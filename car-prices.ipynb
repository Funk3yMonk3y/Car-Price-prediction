{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lineare Regressionsmodelle\nIn dieser Übung versuchen wir den Verkaufspreis von Autos mittles verschiedener Regressionsmodelle vorherzusagen. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:46:55.864358Z","iopub.execute_input":"2022-07-02T12:46:55.865072Z","iopub.status.idle":"2022-07-02T12:46:55.869517Z","shell.execute_reply.started":"2022-07-02T12:46:55.865032Z","shell.execute_reply":"2022-07-02T12:46:55.868289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/car-price-prediction/CarPrice_Assignment.csv\", index_col=\"car_ID\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:46:58.43862Z","iopub.execute_input":"2022-07-02T12:46:58.439245Z","iopub.status.idle":"2022-07-02T12:46:58.45209Z","shell.execute_reply.started":"2022-07-02T12:46:58.439211Z","shell.execute_reply":"2022-07-02T12:46:58.450914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Vorbereitung der Daten\nIm ersten Teil beschäftigen wir uns mit der Vorbereitung der Daten. Das Datenset hat viele Features, in dieser Übung benutzen wir allerdings nur ein paar davon.\n\nZuerst erstellen wir einen neuen `DataFrame` mit den Features `curbweight, enginesize, highwaympg, horsepower, citympg, peakrpm` und `price`.","metadata":{}},{"cell_type":"code","source":"data = df[['curbweight', 'enginesize', 'highwaympg', 'horsepower', 'citympg', 'peakrpm', 'price']]","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:47:00.702609Z","iopub.execute_input":"2022-07-02T12:47:00.703282Z","iopub.status.idle":"2022-07-02T12:47:00.710226Z","shell.execute_reply.started":"2022-07-02T12:47:00.703229Z","shell.execute_reply":"2022-07-02T12:47:00.709357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Benutze `pandas.plotting.scatter_matrix` um einen tieferen Einblick in die Daten zu bekommen. Gibt es Features, welche nützlicher sind als andere?\n- Berechne eine Korrelationsmatrix. Benutze dafür `DataFrame.corr()`.","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import scatter_matrix","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:47:03.209432Z","iopub.execute_input":"2022-07-02T12:47:03.209951Z","iopub.status.idle":"2022-07-02T12:47:03.218111Z","shell.execute_reply.started":"2022-07-02T12:47:03.209897Z","shell.execute_reply":"2022-07-02T12:47:03.216866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.plotting.scatter_matrix(data)\ndata.corr()\ndf.corr()\ndf.info\ndata.info\ndata.info(verbose = True, show_counts = True)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:47:05.736971Z","iopub.execute_input":"2022-07-02T12:47:05.737476Z","iopub.status.idle":"2022-07-02T12:47:08.620641Z","shell.execute_reply.started":"2022-07-02T12:47:05.737444Z","shell.execute_reply":"2022-07-02T12:47:08.619878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zuletzt wollen wir unser Datenset in ein *Feature*-Datenset und ein *Target* Datenset teilen, um anschließend ein Trainings- und ein Testset zu erstellen. Diese können wir diese beiden Sets verwenden um einige Regressionsmodelle zu testen: ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = data[['curbweight', 'enginesize', 'highwaympg', 'horsepower', 'citympg', 'peakrpm']]\ny = data.price\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:47:12.017989Z","iopub.execute_input":"2022-07-02T12:47:12.018484Z","iopub.status.idle":"2022-07-02T12:47:12.965003Z","shell.execute_reply.started":"2022-07-02T12:47:12.01845Z","shell.execute_reply":"2022-07-02T12:47:12.963992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2a. Lineare Regression\n- Benutze `X_train, y_train` und `sklearn.linear_model.LinearRegression` um eine lineare Regression zu trainieren.\n- Validiere das Modell mit `sklearn.model_selection.cross_val_score`. Solltest du das Trainingsset oder das Testset verwenden?\n- Benutze `numpy.mean` um den durchschnittlichen Score zu berechnen.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\n#intercept_ is the expected mean value of y when all X are 0\n#coef_ is used to estimate the coefficients of a linear regression problem\n#cross_val_score is messuring the accuracy\n#np.mean compute the arithmetric mean\n# we should use the train set only, or else the model will be biased\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\ny_predt = lin_reg.predict(X_train)\nprint(y_predt)\nprint(lin_reg.intercept_)\nprint(lin_reg.coef_)\nprint(\"Model validation\",cross_val_score(lin_reg, X_train, y_train))\nprint(\"Durchschnittlicher Score \",np.mean(cross_val_score(lin_reg, X_train, y_train)))\ncompare = pd.DataFrame({'Truevaly': y_train, 'Predictedvaly' : y_predt})\nprint(compare)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:47:23.745925Z","iopub.execute_input":"2022-07-02T12:47:23.746645Z","iopub.status.idle":"2022-07-02T12:47:24.078184Z","shell.execute_reply.started":"2022-07-02T12:47:23.746601Z","shell.execute_reply":"2022-07-02T12:47:24.077071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:48:57.607695Z","iopub.execute_input":"2022-07-02T12:48:57.608057Z","iopub.status.idle":"2022-07-02T12:48:57.613016Z","shell.execute_reply.started":"2022-07-02T12:48:57.608024Z","shell.execute_reply":"2022-07-02T12:48:57.611924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2b. Polynomregression\nKreiere eine `Pipeline` und benutze `PolynomialFeatures` um ein Polynomregressions-Modell zu erstellen.\nBerechne den durchschnittlichen Cross Validation Score.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nimport numpy\nstd_scaler = StandardScaler()\npolybig_features=PolynomialFeatures(degree = 2, include_bias=False).fit(X_train,y_train)\npoly_reg = Pipeline([('poly_features',polybig_features),('std_scaler', std_scaler),('lin_reg',lin_reg)])\npoly_reg.fit(X_train,y_train)\nprint(cross_val_score(poly_reg, X_train, y_train))\nprint(\"Durchschnittlicher cross_val_score\",np.mean(cross_val_score(poly_reg, X_train, y_train)))","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:48:59.684931Z","iopub.execute_input":"2022-07-02T12:48:59.685256Z","iopub.status.idle":"2022-07-02T12:48:59.781811Z","shell.execute_reply.started":"2022-07-02T12:48:59.685228Z","shell.execute_reply":"2022-07-02T12:48:59.780594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wir können eine Cross Validation mittles `GridSearchCV` machen. Um die inneren Parameter der `Pipeline` zu variieren können wir einen doppelten Unterstrich verwenden: `poly_features__degree`.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(poly_reg, {'poly_features__degree': [1, 2, 3]})\ngrid_search.fit(X_train, y_train)\nprint(cross_val_score(grid_search,X_train,y_train))\nprint(f'best parameter {grid_search.best_params_}')","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:49:06.466075Z","iopub.execute_input":"2022-07-02T12:49:06.466438Z","iopub.status.idle":"2022-07-02T12:49:07.372868Z","shell.execute_reply.started":"2022-07-02T12:49:06.466404Z","shell.execute_reply":"2022-07-02T12:49:07.371158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"polyvar_features = PolynomialFeatures","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:49:10.294169Z","iopub.execute_input":"2022-07-02T12:49:10.294676Z","iopub.status.idle":"2022-07-02T12:49:10.299415Z","shell.execute_reply.started":"2022-07-02T12:49:10.29464Z","shell.execute_reply":"2022-07-02T12:49:10.298413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2c. Ridge Regression\n- Erstelle eine `Pipeline` und benutze `PolynomialFeatures` um ein Polynomregressions-Modell zu erstellen. Benutze diesmal eine `Ridge` Regression. \n- Benutze `GridSearchCV` um den Hyperparameter $\\alpha$ zu bestimmen.\n- Berechne den durchschnittlichen Cross Validation Score mit dem besten $\\alpha$-Wert. \n- Regularisierte Modelle funktionieren oft besser, wenn man die Daten zusätzlich skaliert. Benutze `StandardScaler` und vergleiche den Cross Validation Score. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\n\n\npolybig_features = PolynomialFeatures(degree=2, include_bias=False).fit(X_train,y_train)\nstd_scaler = StandardScaler()\nridge_reg = Pipeline([('poly_features',polybig_features),('regul_reg',Ridge())])\nridge_reg.fit(X_test,y_test)\ngrid_search = GridSearchCV(ridge_reg,{'regul_reg__alpha': [x for x in range (0,100)]})\ngrid_search.fit(X_train,y_train)\nprint(f'best parameter {grid_search.best_params_}')\nprint(cross_val_score(ridge_reg,X_train,y_train))\nprint(np.mean(cross_val_score(ridge_reg, X_train, y_train)))\n\nridge_regscale = Pipeline([('poly_features',polybig_features),('std_scaler', std_scaler),('regul_regscaler',Ridge())])\nridge_regscale.fit(X_test,y_test)\ngrid_searchscaler = GridSearchCV(ridge_regscale,{'regul_regscaler__alpha': [x for x in range (0,100)]})\ngrid_searchscaler.fit(X_train,y_train)\nprint(f'best parameter {grid_searchscaler.best_params_}')\nprint(cross_val_score(ridge_regscale,X_train,y_train))\n\nridge_regwcalcalpha = Pipeline([('poly_features',polybig_features),('regul_regscaler',Ridge(alpha=42))])\nridge_regwcalcalpha.fit(X_train,y_train)\nprint(np.mean(cross_val_score(ridge_regwcalcalpha, X_train, y_train)))\nridge_regscalewcalcalpha = Pipeline([('poly_features',polybig_features),('std_scaler', std_scaler),('regul_regscaler',Ridge(alpha=1))])\nridge_regscalewcalcalpha.fit(X_train,y_train)\nprint(np.mean(cross_val_score(ridge_regscalewcalcalpha, X_train, y_train)))\n\ncompare = pd.DataFrame({np.mean(cross_val_score(ridge_regwcalcalpha, X_train, y_train)),np.mean(cross_val_score(ridge_regscalewcalcalpha, X_train, y_train))})\nprint(compare)\n\n#LinAlgWarning occures because the Standardscaler was not included in the ridge_reg Pipeline on purpose for comparison","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:59:29.128495Z","iopub.execute_input":"2022-07-02T12:59:29.128836Z","iopub.status.idle":"2022-07-02T12:59:42.472749Z","shell.execute_reply.started":"2022-07-02T12:59:29.128804Z","shell.execute_reply":"2022-07-02T12:59:42.471329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:59:42.480276Z","iopub.execute_input":"2022-07-02T12:59:42.484123Z","iopub.status.idle":"2022-07-02T12:59:42.494273Z","shell.execute_reply.started":"2022-07-02T12:59:42.484041Z","shell.execute_reply":"2022-07-02T12:59:42.492853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2d. Elastic Net\nBenutze `PolynomialFeatures` mit einem höheren Grad ($\\gt 2$) und erstelle ein Elastic Net Modell. \n- Benutze dafür `SGDRegressor` mit `penalty='elasticnet'`. Benutze weiters `GridSearchCV` um die Hyperparameter `alpha` und `l1_ratio` zu wählen. \n- Was sind die besten Hyperparameter. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolybig_features = PolynomialFeatures(degree=4, include_bias=False).fit(X_test,y_test)\nstd_scaler = StandardScaler()\nelastic_net = Pipeline([\n    (\"poly_features\", polybig_features),\n    (\"std_scaler\", std_scaler),\n    (\"elastic_net\", SGDRegressor(penalty='elasticnet'))])\n\nelastic_net.fit(X_train, y_train)\ngrid_search = GridSearchCV(elastic_net,{'elastic_net__alpha': [x/100 for x in range (0,100)],'elastic_net__l1_ratio': [0,0.05,0.1,0.2,0.5,0.8,0.9,1]}, cv=10)\ngrid_search.fit(X_train,y_train)\nprint(f'best parameter {grid_search.best_params_}')\n\ncv_results = cross_validate(elastic_net, X, y, cv = 3)\nsorted(cv_results.keys())\nprint(cross_val_score(elastic_net, X_train, y_train).mean())\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:59:42.503068Z","iopub.execute_input":"2022-07-02T12:59:42.507041Z","iopub.status.idle":"2022-07-02T13:01:34.285295Z","shell.execute_reply.started":"2022-07-02T12:59:42.50696Z","shell.execute_reply":"2022-07-02T13:01:34.284273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"grid_search = GridSearchCV(elastic_net, {'elastic_net__alpha': [x for x in range (0,100)]}, cv=10)\ngrid_search.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:01:48.058156Z","iopub.execute_input":"2022-07-02T13:01:48.05868Z","iopub.status.idle":"2022-07-02T13:02:01.529845Z","shell.execute_reply.started":"2022-07-02T13:01:48.058644Z","shell.execute_reply":"2022-07-02T13:02:01.529133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wir können die Ergebnisse der Cross Validation in einem `DataFrame` zusammenfassen:","metadata":{}},{"cell_type":"code","source":"outcome = pd.DataFrame(grid_search.cv_results_).sort_values(\"rank_test_score\").head()\nprint(outcome)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:02:13.965142Z","iopub.execute_input":"2022-07-02T13:02:13.965603Z","iopub.status.idle":"2022-07-02T13:02:13.987774Z","shell.execute_reply.started":"2022-07-02T13:02:13.965571Z","shell.execute_reply":"2022-07-02T13:02:13.986981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2e. Early Stopping\nErstelle ein regularisiertes Modell, benutze aber diesmal [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html?highlight=sgdregressor#sklearn-linear-model-sgdregressor) mit der Einstellung `early_stopping=True` für die Implementierung. \n- Benutze `GridSearchCV` um die Lernrate `eta0` zu bestimmen. \n- Erstelle wie zuvor einen `DataFrame` der Ergebnisse. \n- Im Falle eines schlechten *Scores*, versuche den Parameter `n_iter_no_change` zu erhöhen. Was macht dieser Parameter?","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.exceptions import ConvergenceWarning\n\nsgd_reg = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=4)),\n    (\"std_scaler\", StandardScaler()),\n    (\"sgd_reg\", SGDRegressor(early_stopping = True))\n])\n\nsgd_reg.fit(X_train, y_train)\ngrid_searcheta0 = GridSearchCV(sgd_reg,{'sgd_reg__eta0': [x for x in range (0,100)],'sgd_reg__n_iter_no_change' : [x for x in range (1,10)]}, cv=10)\nsgd_reg.get_params().keys()\ncv_results = cross_validate(sgd_reg, X, y, cv = 3)\nsorted(cv_results.keys())\nresult = pd.DataFrame(grid_search.cv_results_).sort_values(\"rank_test_score\").head()\nprint(result)\n#wowo = pd.DataFrame(grid_searcheta0.cv_results_).sort_values(\"rank_test_score\").head()\n#print(wowo)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:02:19.475721Z","iopub.execute_input":"2022-07-02T13:02:19.47612Z","iopub.status.idle":"2022-07-02T13:02:19.624362Z","shell.execute_reply.started":"2022-07-02T13:02:19.476086Z","shell.execute_reply":"2022-07-02T13:02:19.623078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2f*. Feature Engineering (Zusatzaufgabe)\nWir wollen ein Feature `CarMake` hinzufügen. Der ursprüngliche Datensatz hat ein Feature `CarName`. Aus diesem Feature wollen wir die Automarke extrahieren. Allerdings sind die Daten fehlerhaft und können Rechtschreibfehler beinhalten.\n- Wie viele Automarken gibt es im Datenset?","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nmake_mapping = {\n    \"maxda\": \"mazda\",\n    \"Nissan\": \"nissan\",\n    \"porcshce\": \"porsche\",\n    \"toyouta\": \"toyota\",\n    \"vokswagen\": \"vw\",\n    \"volkswagen\": \"vw\",\n}\n\ndef get_make(car_name: str):\n    make = car_name.split(' ')[0]\n    return make_mapping.get(make, make)\n\nfeatures = ['curbweight', 'enginesize', 'highwaympg', 'horsepower', 'citympg', 'peakrpm']\nX = data.loc[:, features]  # Erstellt eine Kopie\ny = data.price.copy()\n\nX[\"CarMake\"] = df.CarName.map(get_make)\nprint(X[\"CarMake\"].value_counts(dropna=False))\nCounter(X[\"CarMake\"])\nprint(\"Es gibt im DataSet 22 verschiedene Automarken\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:02:24.673864Z","iopub.execute_input":"2022-07-02T13:02:24.67418Z","iopub.status.idle":"2022-07-02T13:02:24.686241Z","shell.execute_reply.started":"2022-07-02T13:02:24.674152Z","shell.execute_reply":"2022-07-02T13:02:24.685112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Benutze `sklearn.compose.ColumnTransformer(..., remainder='passthrough'`) und `sklearn.preprocessing.OneHotEncoder` um das kategorische Feature `CarName` in mehrere numerische Features zu verwandeln.\n- Benutze `PolynomialFeatures` und anschließend `StandardScaler` und trainiere ein lineares Regressionsmodell deiner Wahl.","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\n\ndunno_reg = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree = 4)),\n    (\"std_scaler\", StandardScaler),\n    (\"onehotencoder\", OneHotEncoder)\n    \n])","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:02:36.860515Z","iopub.execute_input":"2022-07-02T13:02:36.861134Z","iopub.status.idle":"2022-07-02T13:02:36.867812Z","shell.execute_reply.started":"2022-07-02T13:02:36.861096Z","shell.execute_reply":"2022-07-02T13:02:36.866774Z"},"trusted":true},"execution_count":null,"outputs":[]}]}